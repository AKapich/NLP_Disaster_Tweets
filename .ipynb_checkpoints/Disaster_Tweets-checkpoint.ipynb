{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d96d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "940fbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2e664c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07686e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120dfe3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "187a968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "288b7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    df['tokenized_text'] =  df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b16ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(train)\n",
    "tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "befd0757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, are, the, Reason, of, this, #, ea...\n",
       "1        [Forest, fire, near, La, Ronge, Sask, ., Canada]\n",
       "2       [All, residents, asked, to, 'shelter, in, plac...\n",
       "3       [13,000, people, receive, #, wildfires, evacua...\n",
       "4       [Just, got, sent, this, photo, from, Ruby, #, ...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [@, aria_ahrary, @, TheTawniest, The, out, of,...\n",
       "7610    [M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...\n",
       "7611    [Police, investigating, after, an, e-bike, col...\n",
       "7612    [The, Latest, :, More, Homes, Razed, by, North...\n",
       "Name: tokenized_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokenized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdc088",
   "metadata": {},
   "source": [
    "## Removing punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19febc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0c95695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df):\n",
    "    df['only_text'] = df['tokenized_text'].apply(\n",
    "        lambda row: [word for word in row if word not in punctuation]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c52fa2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation(train)\n",
    "remove_punctuation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0a2bdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, are, the, Reason, of, this, earth...\n",
       "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
       "2       [All, residents, asked, to, 'shelter, in, plac...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [Just, got, sent, this, photo, from, Ruby, Ala...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [aria_ahrary, TheTawniest, The, out, of, contr...\n",
       "7610    [M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...\n",
       "7611    [Police, investigating, after, an, e-bike, col...\n",
       "7612    [The, Latest, More, Homes, Razed, by, Northern...\n",
       "Name: only_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['only_text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bd623",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc5eec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97559cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    df['cleaned_text'] = df['only_text'].apply(\n",
    "        lambda row: [word for word in row if word not in stop_words]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cdc11637",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(train)\n",
    "remove_stopwords(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6aa98b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, Reason, earthquake, May, ALLAH, F...\n",
       "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
       "2       [All, residents, asked, 'shelter, place, notif...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [Just, got, sent, photo, Ruby, Alaska, smoke, ...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [aria_ahrary, TheTawniest, The, control, wild,...\n",
       "7610    [M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...\n",
       "7611    [Police, investigating, e-bike, collided, car,...\n",
       "7612    [The, Latest, More, Homes, Razed, Northern, Ca...\n",
       "Name: cleaned_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6f36e",
   "metadata": {},
   "source": [
    "# Extracting more information from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "898e15c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>only_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>[All, residents, asked, 'shelter, place, notif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[Two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[Two, giant, cranes, holding, bridge, collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@, aria_ahrary, @, TheTawniest, The, out, of,...</td>\n",
       "      <td>[aria_ahrary, TheTawniest, The, out, of, contr...</td>\n",
       "      <td>[aria_ahrary, TheTawniest, The, control, wild,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...</td>\n",
       "      <td>[M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...</td>\n",
       "      <td>[M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[Police, investigating, e-bike, collided, car,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, Latest, :, More, Homes, Razed, by, North...</td>\n",
       "      <td>[The, Latest, More, Homes, Razed, by, Northern...</td>\n",
       "      <td>[The, Latest, More, Homes, Razed, Northern, Ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "0     [Our, Deeds, are, the, Reason, of, this, #, ea...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2     [All, residents, asked, to, 'shelter, in, plac...   \n",
       "3     [13,000, people, receive, #, wildfires, evacua...   \n",
       "4     [Just, got, sent, this, photo, from, Ruby, #, ...   \n",
       "...                                                 ...   \n",
       "7608  [Two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [@, aria_ahrary, @, TheTawniest, The, out, of,...   \n",
       "7610  [M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...   \n",
       "7611  [Police, investigating, after, an, e-bike, col...   \n",
       "7612  [The, Latest, :, More, Homes, Razed, by, North...   \n",
       "\n",
       "                                              only_text  \\\n",
       "0     [Our, Deeds, are, the, Reason, of, this, earth...   \n",
       "1         [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2     [All, residents, asked, to, 'shelter, in, plac...   \n",
       "3     [13,000, people, receive, wildfires, evacuatio...   \n",
       "4     [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
       "...                                                 ...   \n",
       "7608  [Two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [aria_ahrary, TheTawniest, The, out, of, contr...   \n",
       "7610  [M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...   \n",
       "7611  [Police, investigating, after, an, e-bike, col...   \n",
       "7612  [The, Latest, More, Homes, Razed, by, Northern...   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0     [Our, Deeds, Reason, earthquake, May, ALLAH, F...  \n",
       "1         [Forest, fire, near, La, Ronge, Sask, Canada]  \n",
       "2     [All, residents, asked, 'shelter, place, notif...  \n",
       "3     [13,000, people, receive, wildfires, evacuatio...  \n",
       "4     [Just, got, sent, photo, Ruby, Alaska, smoke, ...  \n",
       "...                                                 ...  \n",
       "7608  [Two, giant, cranes, holding, bridge, collapse...  \n",
       "7609  [aria_ahrary, TheTawniest, The, control, wild,...  \n",
       "7610  [M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...  \n",
       "7611  [Police, investigating, e-bike, collided, car,...  \n",
       "7612  [The, Latest, More, Homes, Razed, Northern, Ca...  \n",
       "\n",
       "[7613 rows x 8 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "492aac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(df):\n",
    "    df['#_amount'] = df['tokenized_text'].apply(lambda row: len([word for word in row if word=='#']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6922981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions(df):\n",
    "    df['@_amount'] = df['tokenized_text'].apply(lambda row: len([word for word in row if word=='@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6168dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(df):\n",
    "    df['punctuation_amount'] = df['tokenized_text'].apply(\n",
    "        lambda row: len([word for word in row if word in punctuation])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2c243b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(df):\n",
    "    df['stopword_amount'] = df['tokenized_text'].apply(\n",
    "        lambda row: len([word for word in row if word in stop_words])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b020d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_urls(df):\n",
    "    df['url_amount'] = df['tokenized_text'].apply(\n",
    "        lambda row: len([word for word in row if 'http' in word or 'https' in word])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b65fba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_length(df):\n",
    "    df['mean_word_length'] = df['only_text'].apply(\n",
    "        lambda row: round(np.mean([len(word) for word in row]), 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb4a0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_amount(df):\n",
    "    df['word_amount'] = df['only_text'].apply(\n",
    "        lambda row: len(row)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8332b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_amount(df):\n",
    "    df['unique_word_amount'] = df['only_text'].apply(\n",
    "        lambda row: len(list(set(row)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2d8f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_location(df):\n",
    "    df['has_location']  = df['location'].str.len()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32c3e74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase_percentage(text):\n",
    "    return(sum(1 for c in text if c.isupper())/len(text))\n",
    "\n",
    "def count_uppercase_percentage(df):\n",
    "    df['uppercase_percentage'] = df['text'].apply(uppercase_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "202743db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "    count_hashtags(df)\n",
    "    count_mentions(df)\n",
    "    count_punctuation(df)\n",
    "    count_stopwords(df)\n",
    "    count_urls(df)\n",
    "    mean_word_length(df)\n",
    "    word_amount(df)\n",
    "    unique_word_amount(df)\n",
    "    has_location(df)\n",
    "    count_uppercase_percentage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d478500",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(train)\n",
    "extract_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d9723e",
   "metadata": {},
   "source": [
    "# Keyword analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "49cee08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we look for the most common keywords in the tweets\n",
    "top_target_keywords = train[train.target==1].keyword.value_counts().head(100)\n",
    "top_non_target_keywords = train[train.target==0].keyword.value_counts().head(100)\n",
    "\n",
    "words_1 = set(list(top_target_keywords.index))\n",
    "words_0 = set(list(top_non_target_keywords.index))\n",
    "intersection = list(words_1.intersection(words_0)) # identification of ambigual keywords\n",
    "\n",
    "top_target_keywords = top_target_keywords.drop(intersection)\n",
    "top_non_target_keywords = top_non_target_keywords.drop(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1f90d6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7ddc6e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_keywords(df):\n",
    "    df['target_keyword'] = df['keyword'].apply(lambda keyword: keyword in top_target_keywords)\n",
    "    df['non_target_keyword'] = df['keyword'].apply(lambda keyword: keyword in top_non_target_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c1773e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_keywords(train)\n",
    "analyze_keywords(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c0fc9",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a992b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bf0a5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_compound_sentiment(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)['compound']\n",
    "\n",
    "def sentiment(df):\n",
    "    df['sentiment'] = df['text'].apply(calc_compound_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f64d2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment(train)\n",
    "sentiment(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37ff335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# profile = ProfileReport(train.sample(frac=0.1), title=\"Twitter Profiling Report\", explorative=True)\n",
    "# profile.to_file(\"twitter.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2466be1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "46c9f639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>only_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>#_amount</th>\n",
       "      <th>@_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>stopword_amount</th>\n",
       "      <th>url_amount</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>word_amount</th>\n",
       "      <th>unique_word_amount</th>\n",
       "      <th>has_location</th>\n",
       "      <th>uppercase_percentage</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target_keyword</th>\n",
       "      <th>non_target_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.31</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.43</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>[All, residents, asked, 'shelter, place, notif...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[Two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[Two, giant, cranes, holding, bridge, collapse...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.00</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@, aria_ahrary, @, TheTawniest, The, out, of,...</td>\n",
       "      <td>[aria_ahrary, TheTawniest, The, out, of, contr...</td>\n",
       "      <td>[aria_ahrary, TheTawniest, The, control, wild,...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...</td>\n",
       "      <td>[M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...</td>\n",
       "      <td>[M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[Police, investigating, e-bike, collided, car,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6.16</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, Latest, :, More, Homes, Razed, by, North...</td>\n",
       "      <td>[The, Latest, More, Homes, Razed, by, Northern...</td>\n",
       "      <td>[The, Latest, More, Homes, Razed, Northern, Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.08</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "0     [Our, Deeds, are, the, Reason, of, this, #, ea...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2     [All, residents, asked, to, 'shelter, in, plac...   \n",
       "3     [13,000, people, receive, #, wildfires, evacua...   \n",
       "4     [Just, got, sent, this, photo, from, Ruby, #, ...   \n",
       "...                                                 ...   \n",
       "7608  [Two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [@, aria_ahrary, @, TheTawniest, The, out, of,...   \n",
       "7610  [M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...   \n",
       "7611  [Police, investigating, after, an, e-bike, col...   \n",
       "7612  [The, Latest, :, More, Homes, Razed, by, North...   \n",
       "\n",
       "                                              only_text  \\\n",
       "0     [Our, Deeds, are, the, Reason, of, this, earth...   \n",
       "1         [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2     [All, residents, asked, to, 'shelter, in, plac...   \n",
       "3     [13,000, people, receive, wildfires, evacuatio...   \n",
       "4     [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
       "...                                                 ...   \n",
       "7608  [Two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [aria_ahrary, TheTawniest, The, out, of, contr...   \n",
       "7610  [M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...   \n",
       "7611  [Police, investigating, after, an, e-bike, col...   \n",
       "7612  [The, Latest, More, Homes, Razed, by, Northern...   \n",
       "\n",
       "                                           cleaned_text  #_amount  @_amount  \\\n",
       "0     [Our, Deeds, Reason, earthquake, May, ALLAH, F...         1         0   \n",
       "1         [Forest, fire, near, La, Ronge, Sask, Canada]         0         0   \n",
       "2     [All, residents, asked, 'shelter, place, notif...         0         0   \n",
       "3     [13,000, people, receive, wildfires, evacuatio...         1         0   \n",
       "4     [Just, got, sent, photo, Ruby, Alaska, smoke, ...         2         0   \n",
       "...                                                 ...       ...       ...   \n",
       "7608  [Two, giant, cranes, holding, bridge, collapse...         0         0   \n",
       "7609  [aria_ahrary, TheTawniest, The, control, wild,...         0         2   \n",
       "7610  [M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...         0         0   \n",
       "7611  [Police, investigating, e-bike, collided, car,...         0         0   \n",
       "7612  [The, Latest, More, Homes, Razed, Northern, Ca...         0         0   \n",
       "\n",
       "      ...  stopword_amount  url_amount  mean_word_length  word_amount  \\\n",
       "0     ...                5           0              4.31           13   \n",
       "1     ...                0           0              4.43            7   \n",
       "2     ...                9           0              5.00           22   \n",
       "3     ...                1           0              7.00            8   \n",
       "4     ...                6           0              4.38           16   \n",
       "...   ...              ...         ...               ...          ...   \n",
       "7608  ...                2           1              6.00           12   \n",
       "7609  ...                7           0              5.10           20   \n",
       "7610  ...                1           1              5.30           10   \n",
       "7611  ...                5           0              6.16           19   \n",
       "7612  ...                1           1              6.08           13   \n",
       "\n",
       "      unique_word_amount  has_location  uppercase_percentage  sentiment  \\\n",
       "0                     13         False                  0.14       0.27   \n",
       "1                      7         False                  0.13      -0.34   \n",
       "2                     19         False                  0.02      -0.30   \n",
       "3                      8         False                  0.02       0.00   \n",
       "4                     15         False                  0.03       0.00   \n",
       "...                  ...           ...                   ...        ...   \n",
       "7608                  12         False                  0.08      -0.49   \n",
       "7609                  17         False                  0.05      -0.58   \n",
       "7610                  10         False                  0.15       0.00   \n",
       "7611                  19         False                  0.03      -0.78   \n",
       "7612                  13         False                  0.17       0.00   \n",
       "\n",
       "      target_keyword  non_target_keyword  \n",
       "0              False               False  \n",
       "1              False               False  \n",
       "2              False               False  \n",
       "3              False               False  \n",
       "4              False               False  \n",
       "...              ...                 ...  \n",
       "7608           False               False  \n",
       "7609           False               False  \n",
       "7610           False               False  \n",
       "7611           False               False  \n",
       "7612           False               False  \n",
       "\n",
       "[7613 rows x 21 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "32103262",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "X_train = train.drop(['tokenized_text', 'only_text', 'cleaned_text',\n",
    "                      'keyword', 'location', 'text', 'id', 'target'], axis=1)\n",
    "X_test = test.drop(['tokenized_text', 'only_text', 'cleaned_text',\n",
    "                    'keyword', 'location', 'text', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "77d2e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=120)\n",
    "rf_model.fit(X_train, y_train)\n",
    "RandomForestClassifier(n_estimators=120)\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2218e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(test['id'])\n",
    "y_pred = pd.DataFrame ({'id':indices,\n",
    "                        'target':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "05482a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('rf_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e3334",
   "metadata": {},
   "source": [
    "# Lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d218a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "454a5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "23b73b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8023b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:08<00:00,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "85f2428c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
       "Model                                                                           \n",
       "ExtraTreesClassifier               0.71               0.69     0.69      0.70   \n",
       "RandomForestClassifier             0.71               0.69     0.69      0.70   \n",
       "LGBMClassifier                     0.70               0.68     0.68      0.69   \n",
       "XGBClassifier                      0.69               0.68     0.68      0.69   \n",
       "SVC                                0.70               0.67     0.67      0.69   \n",
       "BaggingClassifier                  0.68               0.66     0.66      0.68   \n",
       "NuSVC                              0.67               0.66     0.66      0.67   \n",
       "AdaBoostClassifier                 0.67               0.65     0.65      0.67   \n",
       "KNeighborsClassifier               0.67               0.65     0.65      0.66   \n",
       "GaussianNB                         0.65               0.65     0.65      0.65   \n",
       "LinearDiscriminantAnalysis         0.66               0.64     0.64      0.66   \n",
       "QuadraticDiscriminantAnalysis      0.63               0.64     0.64      0.63   \n",
       "RidgeClassifier                    0.66               0.64     0.64      0.66   \n",
       "RidgeClassifierCV                  0.66               0.64     0.64      0.65   \n",
       "LinearSVC                          0.66               0.64     0.64      0.65   \n",
       "SGDClassifier                      0.66               0.64     0.64      0.65   \n",
       "DecisionTreeClassifier             0.64               0.64     0.64      0.65   \n",
       "NearestCentroid                    0.64               0.64     0.64      0.64   \n",
       "LogisticRegression                 0.66               0.64     0.64      0.65   \n",
       "CalibratedClassifierCV             0.66               0.64     0.64      0.65   \n",
       "LabelSpreading                     0.64               0.64     0.64      0.65   \n",
       "LabelPropagation                   0.64               0.63     0.63      0.64   \n",
       "ExtraTreeClassifier                0.63               0.62     0.62      0.63   \n",
       "BernoulliNB                        0.63               0.62     0.62      0.63   \n",
       "Perceptron                         0.60               0.59     0.59      0.60   \n",
       "PassiveAggressiveClassifier        0.57               0.53     0.53      0.53   \n",
       "DummyClassifier                    0.58               0.50     0.50      0.42   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "ExtraTreesClassifier                 0.65  \n",
       "RandomForestClassifier               0.74  \n",
       "LGBMClassifier                       0.08  \n",
       "XGBClassifier                        0.16  \n",
       "SVC                                  1.23  \n",
       "BaggingClassifier                    0.19  \n",
       "NuSVC                                2.70  \n",
       "AdaBoostClassifier                   0.18  \n",
       "KNeighborsClassifier                 0.08  \n",
       "GaussianNB                           0.01  \n",
       "LinearDiscriminantAnalysis           0.02  \n",
       "QuadraticDiscriminantAnalysis        0.01  \n",
       "RidgeClassifier                      0.01  \n",
       "RidgeClassifierCV                    0.02  \n",
       "LinearSVC                            0.17  \n",
       "SGDClassifier                        0.02  \n",
       "DecisionTreeClassifier               0.04  \n",
       "NearestCentroid                      0.01  \n",
       "LogisticRegression                   0.02  \n",
       "CalibratedClassifierCV               0.69  \n",
       "LabelSpreading                       1.11  \n",
       "LabelPropagation                     0.79  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "BernoulliNB                          0.01  \n",
       "Perceptron                           0.01  \n",
       "PassiveAggressiveClassifier          0.01  \n",
       "DummyClassifier                      0.01  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba276b7",
   "metadata": {},
   "source": [
    "# Further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b8dfd297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derailment     39\n",
       "wreckage       39\n",
       "outbreak       39\n",
       "debris         37\n",
       "oil%20spill    37\n",
       "               ..\n",
       "crushed         4\n",
       "screamed        4\n",
       "obliterate      4\n",
       "drown           3\n",
       "bloody          3\n",
       "Name: keyword, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_target_keywords = train[train.target==1].keyword.value_counts().head(100)\n",
    "top_target_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "893017b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body%20bags          40\n",
       "harm                 37\n",
       "armageddon           37\n",
       "wrecked              36\n",
       "ruin                 36\n",
       "                     ..\n",
       "bush%20fires          7\n",
       "casualties            7\n",
       "drought               7\n",
       "bridge%20collapse     6\n",
       "mass%20murder         5\n",
       "Name: keyword, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_non_target_keywords = train[train.target==0].keyword.value_counts().head(100)\n",
    "top_non_target_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
