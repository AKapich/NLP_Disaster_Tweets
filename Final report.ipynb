{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbdac8d",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3f776",
   "metadata": {},
   "source": [
    "The task was to classify whether tweets concern a natural disaster or not. The dataset comes from https://www.kaggle.com/competitions/nlp-getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad668f9",
   "metadata": {},
   "source": [
    "# Dataset overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310cc1bd",
   "metadata": {},
   "source": [
    "First things first - let's take a loot at the dataset to find out what we have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "06073d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239ef77",
   "metadata": {},
   "source": [
    "The dataset consists of 4 columns, one of which being redundant (id), and the target. One can easily see that `keyword` and `location` are likely to be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09448e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows in total: 7613\n",
      "Rows with no keyword: 61\n",
      "Rows with no location: 2533\n",
      "Rows with no text: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "Rows in total: {len(train_df)}\n",
    "Rows with no keyword: {len(train_df[train_df['keyword'].isna()])}\n",
    "Rows with no location: {len(train_df[train_df['location'].isna()])}\n",
    "Rows with no text: {len(train_df[train_df['text'].isna()])}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe0e30",
   "metadata": {},
   "source": [
    "Fortunately keyword is missing in only 61 rows and the location is present in most of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2a683",
   "metadata": {},
   "source": [
    "# Initial approach - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b91d81",
   "metadata": {},
   "source": [
    "We decided to start off with something simple and try to find an efficient solution that would not involve neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cee83",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a251a6d",
   "metadata": {},
   "source": [
    "As classifiers only operate on numeric values, the first challenge was to convert our text input into valid input. Having no prior experience with NLP, we focused mostly on creating metadata related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d8b7c096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jakublange/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jakublange/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def count_capital_letters_part(text):\n",
    "    capital_letters = 0\n",
    "    c: str\n",
    "    for c in text:\n",
    "        if c.isupper():\n",
    "            capital_letters += 1\n",
    "\n",
    "    return capital_letters / len(text) if len(text) > 0 else 0\n",
    "    \n",
    "def avg_word_length(text):\n",
    "    return mean([len(word) for word in text.split()])\n",
    "\n",
    "def calc_compound_sentiment(text):\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)['compound']\n",
    "\n",
    "\n",
    "def calc_location_credibility(loc):\n",
    "    words = loc.split()\n",
    "\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "\n",
    "    score = 0\n",
    "    for word in words:\n",
    "        if word[0].isupper():\n",
    "            score += 1\n",
    "        else:\n",
    "            score -= 1\n",
    "    \n",
    "    return score / len(words)\n",
    "\n",
    "def calculate_top_keywords(df, y):\n",
    "    top_target_keywords = df[y==1].keyword.value_counts().head(100)\n",
    "    top_non_target_keywords = df[y==0].keyword.value_counts().head(100)\n",
    "\n",
    "    words_1 = set(list(top_target_keywords.index))\n",
    "    words_0 = set(list(top_non_target_keywords.index))\n",
    "    intersection = list(words_1.intersection(words_0)) # identification of ambigual keywords\n",
    "\n",
    "    top_target_keywords = top_target_keywords.drop(intersection)\n",
    "    top_non_target_keywords = top_non_target_keywords.drop(intersection)\n",
    "    \n",
    "    return top_target_keywords, top_non_target_keywords\n",
    "\n",
    "\n",
    "def generate_columns(df):\n",
    "    _df = df.copy()\n",
    "    generated_df = pd.DataFrame()\n",
    "    _df[['keyword', 'location']] = _df[['keyword', 'location']].fillna(value='')\n",
    "    _df['keyword'] = _df['keyword'].str.replace('%20', ' ') # %20 is actually a space but we don't want it\n",
    "    \n",
    "    top_target_keywords, top_non_target_keywords = calculate_top_keywords(X_train, y_train)\n",
    "    \n",
    "    generated_df['text_length'] = _df['text'].str.len()\n",
    "    generated_df['capital_letters_part'] = _df['text'].apply(count_capital_letters_part)\n",
    "    generated_df['avg_word_length'] = _df['text'].apply(avg_word_length)\n",
    "    generated_df['sentiment'] = _df['text'].apply(calc_compound_sentiment)\n",
    "    generated_df['hashtag_count'] = _df['text'].apply(lambda text: text.count('#'))\n",
    "    generated_df['punctuation_marks_per_char'] = _df['text'].apply(lambda text: (text.count('?') + text.count('!') + text.count('.')) / len(text))\n",
    "    generated_df['keyword_sentiment'] = _df['keyword'].apply(calc_compound_sentiment)\n",
    "    generated_df['location_credibility'] = _df['location'].apply(calc_location_credibility)\n",
    "    generated_df['target_keyword'] = _df['keyword'].apply(lambda keyword: keyword in top_target_keywords)\n",
    "    generated_df['non_target_keyword'] = _df['keyword'].apply(lambda keyword: keyword in top_non_target_keywords)\n",
    "\n",
    "    return generated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c98abe",
   "metadata": {},
   "source": [
    "The most important of the functions above is the last one, which does the actual job of creating a new dataframe with numeric values based on the source dataframe.\n",
    "Most of the columns are self-explanatory, except for a few of them:\n",
    "- keyword_sentiment - overall (\"compound\") sentiment of text calculated using `SentimentIntensityAnalyzer` from `nltk` package\n",
    "- location_credibility - this is a factor based on an observation, that strings in which each word starts with a capital letter are more likely to be a real location\n",
    "- target_keyword - tells whether the keyword if the sample is among the top 100 keywords for samples with target equal to 1\n",
    "- non_target_keyword - same as the previous one but for samples with target equal to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe630eda",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61002aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(X):\n",
    "  scaler = StandardScaler()\n",
    "  return scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545afef1",
   "metadata": {},
   "source": [
    "## Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "46fa41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X):\n",
    "    _X = X.copy()\n",
    "    _X = _X.iloc[:, 1:] # we don't want to include id as it should affect predictions\n",
    "    _X = generate_columns(_X)\n",
    "    _X = standardize(_X)\n",
    "    return _X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc185434",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "efc10625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "X = df.iloc[:, :-1] \n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.3)\n",
    "\n",
    "X_train_preprocessed = preprocess(X_train)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c35eb7",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0919c174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78      1303\n",
      "           1       0.72      0.62      0.67       981\n",
      "\n",
      "    accuracy                           0.74      2284\n",
      "   macro avg       0.73      0.72      0.73      2284\n",
      "weighted avg       0.73      0.74      0.73      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_test_preprocessed = preprocess(X_test)\n",
    "y_pred = model.predict(X_test_preprocessed)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b4b87",
   "metadata": {},
   "source": [
    "# Second approach - Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "86b9be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "\n",
    "def tfidf(column: pd.Series) -> tuple[TfidfVectorizer, pd.DataFrame]:\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b[A-Za-z]+\\b', # exclude digits\n",
    "                                 stop_words=list(ENGLISH_STOP_WORDS)) # exclude stop words\n",
    "    sparse_matrix = vectorizer.fit_transform(column.str.lower())\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return vectorizer, pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=feature_names)\n",
    "\n",
    "def generate_tfidf_columns(df) -> tuple[pd.DataFrame, TfidfVectorizer]:\n",
    "    _df = df.copy()\n",
    "    text_vectorizer, text_tfidf = tfidf(_df['text'])\n",
    "    \n",
    "    return text_tfidf, text_vectorizer\n",
    "\n",
    "def apply_vectorizer_to_test(X_test: pd.DataFrame, text_vectorizer) -> pd.DataFrame:   \n",
    "    vectorized_text = text_vectorizer.transform(X_test['text'])\n",
    "    X_text = pd.DataFrame.sparse.from_spmatrix(vectorized_text, \n",
    "                                               columns=text_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d12fd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf, text_vectorizer = generate_tfidf_columns(X_train)\n",
    "\n",
    "tfidf_classifier = RandomForestClassifier()\n",
    "\n",
    "tfidf_classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba54af5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_tfidf = apply_vectorizer_to_test(X_test, text_vectorizer)\n",
    "y_pred_tfidf = tfidf_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e6a005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.93      0.83      1303\n",
      "           1       0.86      0.59      0.70       981\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.80      0.76      0.77      2284\n",
      "weighted avg       0.80      0.78      0.77      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9abcc",
   "metadata": {},
   "source": [
    "Tfidf seemes to be slightly more accurate than the previous approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
