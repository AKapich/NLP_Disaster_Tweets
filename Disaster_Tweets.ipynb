{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d96d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "940fbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a2e664c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07686e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120dfe3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "187a968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "288b7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    df['tokenized_text'] =  df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b16ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(train)\n",
    "tokenize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "befd0757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, are, the, Reason, of, this, #, ea...\n",
       "1        [Forest, fire, near, La, Ronge, Sask, ., Canada]\n",
       "2       [All, residents, asked, to, 'shelter, in, plac...\n",
       "3       [13,000, people, receive, #, wildfires, evacua...\n",
       "4       [Just, got, sent, this, photo, from, Ruby, #, ...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [@, aria_ahrary, @, TheTawniest, The, out, of,...\n",
       "7610    [M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...\n",
       "7611    [Police, investigating, after, an, e-bike, col...\n",
       "7612    [The, Latest, :, More, Homes, Razed, by, North...\n",
       "Name: tokenized_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tokenized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdc088",
   "metadata": {},
   "source": [
    "## Removing punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19febc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0c95695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(df):\n",
    "    df['only_text'] = df['tokenized_text'].apply(\n",
    "        lambda row: [word for word in row if word not in punctuation]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c52fa2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation(train)\n",
    "remove_punctuation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0a2bdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, are, the, Reason, of, this, earth...\n",
       "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
       "2       [All, residents, asked, to, 'shelter, in, plac...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [Just, got, sent, this, photo, from, Ruby, Ala...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [aria_ahrary, TheTawniest, The, out, of, contr...\n",
       "7610    [M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...\n",
       "7611    [Police, investigating, after, an, e-bike, col...\n",
       "7612    [The, Latest, More, Homes, Razed, by, Northern...\n",
       "Name: only_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['only_text'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1bd623",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc5eec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97559cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df):\n",
    "    df['cleaned_text'] = df['only_text'].apply(\n",
    "        lambda row: [word for word in row if word not in stop_words]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdc11637",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords(train)\n",
    "remove_stopwords(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6aa98b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [Our, Deeds, Reason, earthquake, May, ALLAH, F...\n",
       "1           [Forest, fire, near, La, Ronge, Sask, Canada]\n",
       "2       [All, residents, asked, 'shelter, place, notif...\n",
       "3       [13,000, people, receive, wildfires, evacuatio...\n",
       "4       [Just, got, sent, photo, Ruby, Alaska, smoke, ...\n",
       "                              ...                        \n",
       "7608    [Two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [aria_ahrary, TheTawniest, The, control, wild,...\n",
       "7610    [M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...\n",
       "7611    [Police, investigating, e-bike, collided, car,...\n",
       "7612    [The, Latest, More, Homes, Razed, Northern, Ca...\n",
       "Name: cleaned_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6f36e",
   "metadata": {},
   "source": [
    "# Extracting more information from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "898e15c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>only_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
       "      <td>[Our, Deeds, Reason, earthquake, May, ALLAH, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td>[All, residents, asked, 'shelter, place, notif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #, wildfires, evacua...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
       "      <td>[Just, got, sent, photo, Ruby, Alaska, smoke, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[Two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "      <td>[Two, giant, cranes, holding, bridge, collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[@, aria_ahrary, @, TheTawniest, The, out, of,...</td>\n",
       "      <td>[aria_ahrary, TheTawniest, The, out, of, contr...</td>\n",
       "      <td>[aria_ahrary, TheTawniest, The, control, wild,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...</td>\n",
       "      <td>[M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...</td>\n",
       "      <td>[M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[Police, investigating, after, an, e-bike, col...</td>\n",
       "      <td>[Police, investigating, e-bike, collided, car,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, Latest, :, More, Homes, Razed, by, North...</td>\n",
       "      <td>[The, Latest, More, Homes, Razed, by, Northern...</td>\n",
       "      <td>[The, Latest, More, Homes, Razed, Northern, Ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "0     [Our, Deeds, are, the, Reason, of, this, #, ea...   \n",
       "1      [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2     [All, residents, asked, to, 'shelter, in, plac...   \n",
       "3     [13,000, people, receive, #, wildfires, evacua...   \n",
       "4     [Just, got, sent, this, photo, from, Ruby, #, ...   \n",
       "...                                                 ...   \n",
       "7608  [Two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [@, aria_ahrary, @, TheTawniest, The, out, of,...   \n",
       "7610  [M1.94, [, 01:04, UTC, ], ?, 5km, S, of, Volca...   \n",
       "7611  [Police, investigating, after, an, e-bike, col...   \n",
       "7612  [The, Latest, :, More, Homes, Razed, by, North...   \n",
       "\n",
       "                                              only_text  \\\n",
       "0     [Our, Deeds, are, the, Reason, of, this, earth...   \n",
       "1         [Forest, fire, near, La, Ronge, Sask, Canada]   \n",
       "2     [All, residents, asked, to, 'shelter, in, plac...   \n",
       "3     [13,000, people, receive, wildfires, evacuatio...   \n",
       "4     [Just, got, sent, this, photo, from, Ruby, Ala...   \n",
       "...                                                 ...   \n",
       "7608  [Two, giant, cranes, holding, a, bridge, colla...   \n",
       "7609  [aria_ahrary, TheTawniest, The, out, of, contr...   \n",
       "7610  [M1.94, 01:04, UTC, 5km, S, of, Volcano, Hawai...   \n",
       "7611  [Police, investigating, after, an, e-bike, col...   \n",
       "7612  [The, Latest, More, Homes, Razed, by, Northern...   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0     [Our, Deeds, Reason, earthquake, May, ALLAH, F...  \n",
       "1         [Forest, fire, near, La, Ronge, Sask, Canada]  \n",
       "2     [All, residents, asked, 'shelter, place, notif...  \n",
       "3     [13,000, people, receive, wildfires, evacuatio...  \n",
       "4     [Just, got, sent, photo, Ruby, Alaska, smoke, ...  \n",
       "...                                                 ...  \n",
       "7608  [Two, giant, cranes, holding, bridge, collapse...  \n",
       "7609  [aria_ahrary, TheTawniest, The, control, wild,...  \n",
       "7610  [M1.94, 01:04, UTC, 5km, S, Volcano, Hawaii, h...  \n",
       "7611  [Police, investigating, e-bike, collided, car,...  \n",
       "7612  [The, Latest, More, Homes, Razed, Northern, Ca...  \n",
       "\n",
       "[7613 rows x 8 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "492aac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(df):\n",
    "    df['#_amount'] = df['tokenized_text'].apply(lambda row: len([word for word in row if word=='#']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c76c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hashtags(train)\n",
    "count_hashtags(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6922981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions(df):\n",
    "    df['@_amount'] = df['tokenized_text'].apply(lambda row: len([word for word in row if word=='@']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8b7bb725",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mentions(train)\n",
    "count_mentions(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6168dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(df):\n",
    "    df['punctuation_amount'] = df['tokenized_text'].apply(\n",
    "        lambda row: len([word for word in row if word in punctuation])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b5f9c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_punctuation(train)\n",
    "count_punctuation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b2c243b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(df):\n",
    "    df['stopword_amount'] = df['tokenized_text'].apply(\n",
    "        lambda row: len([word for word in row if word in stop_words])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f230823",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_stopwords(train)\n",
    "count_stopwords(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b020d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_urls(df):\n",
    "    df['url_amount'] = df['tokenized_text'].apply(\n",
    "        lambda row: len([word for word in row if 'http' in word or 'https' in word])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98e2cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_urls(train)\n",
    "count_urls(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b65fba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_word_length(df):\n",
    "    df['mean_word_length'] = df['only_text'].apply(\n",
    "        lambda row: round(np.mean([len(word) for word in row]), 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e310113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_word_length(train)\n",
    "mean_word_length(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb4a0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_amount(df):\n",
    "    df['word_amount'] = df['only_text'].apply(\n",
    "        lambda row: len(row)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0828cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_amount(train)\n",
    "word_amount(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8332b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_amount(df):\n",
    "    df['unique_word_amount'] = df['only_text'].apply(\n",
    "        lambda row: len(list(set(row)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac6e893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_amount(train)\n",
    "unique_word_amount(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2d8f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_location(df):\n",
    "    df['has_location']  = df['location'].str.len()>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b017c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_location(train)\n",
    "has_location(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a992b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf0a5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_compound_sentiment(text):\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)['compound']\n",
    "\n",
    "def sentiment(df):\n",
    "    df['sentiment'] = df['text'].apply(calc_compound_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f64d2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment(train)\n",
    "sentiment(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ac35865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase_percentage(text):\n",
    "    return(sum(1 for c in text if c.isupper())/len(text))\n",
    "\n",
    "def count_uppercase_percentage(df):\n",
    "    df['uppercase_percentage'] = df['text'].apply(uppercase_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0765149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_uppercase_percentage(train)\n",
    "count_uppercase_percentage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37ff335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104b06fd97044723a99c2c4d0ae53b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab3b2fb146c4e10b261665b4b994faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5341e5a638934e8f9e059fb9ef456b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab83121a90284690a9ff559a90568099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# profile = ProfileReport(train.sample(frac=0.1), title=\"Twitter Profiling Report\", explorative=True)\n",
    "# profile.to_file(\"twitter.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2466be1",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "32103262",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "X_train = train.drop(['tokenized_text', 'only_text', 'cleaned_text',\n",
    "                      'keyword', 'location', 'text', 'target', 'id'], axis=1)\n",
    "X_test = test.drop(['tokenized_text', 'only_text', 'cleaned_text',\n",
    "                    'keyword', 'location', 'text', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77d2e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=120)\n",
    "rf_model.fit(X_train, y_train)\n",
    "RandomForestClassifier(n_estimators=120)\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2218e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(test['id'])\n",
    "y_pred = pd.DataFrame ({'id':indices,\n",
    "                        'target':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05482a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e3334",
   "metadata": {},
   "source": [
    "# Lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d218a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "454a5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23b73b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8023b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:10<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba276b7",
   "metadata": {},
   "source": [
    "# Further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b8dfd297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derailment           39\n",
       "wreckage             39\n",
       "outbreak             39\n",
       "debris               37\n",
       "oil%20spill          37\n",
       "typhoon              37\n",
       "evacuated            32\n",
       "suicide%20bombing    32\n",
       "rescuers             32\n",
       "suicide%20bomb       32\n",
       "Name: keyword, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_target_keywords = train[train.target==1].keyword.value_counts()\n",
    "top_target_keywords.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "893017b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body%20bags    40\n",
       "harm           37\n",
       "armageddon     37\n",
       "wrecked        36\n",
       "ruin           36\n",
       "deluge         36\n",
       "explode        35\n",
       "twister        35\n",
       "fear           35\n",
       "siren          35\n",
       "Name: keyword, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_non_target_keywords = train[train.target==0].keyword.value_counts()\n",
    "top_non_target_keywords.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
